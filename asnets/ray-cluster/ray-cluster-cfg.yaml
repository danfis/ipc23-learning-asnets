cluster_name: asnets

min_workers: 0
# a reasonable number of workers is good, but having too many usually means that
# it autoscales fast during load spikes & then takes ages to autoscale down once
# the spikes pass, which probably costs a bit of money
max_workers: 3
initial_workers: 0
autoscaling_mode: default

# This executes all commands on all nodes in the docker container,
# and opens all the necessary ports to support the Ray cluster.
# Empty string means disabled.
docker:
    image: "" # e.g., tensorflow/tensorflow:1.5.0-py3
    container_name: "" # e.g. ray_docker
    run_options: []  # Extra options to pass into "docker run"

# The autoscaler will scale up the cluster to this target fraction of resource
# usage. Must be <1.0 for scaling to happen.
target_utilization_fraction: 0.9
idle_timeout_minutes: 5

# Cloud-provider specific configuration.
provider:
    type: aws
    region: us-west-2
    # Availability zone(s), comma-separated, that nodes may be launched in.
    # Nodes are currently spread between zones by a round-robin approach,
    # however this implementation detail should not be relied upon.
    availability_zone: us-west-2a,us-west-2b,us-west-2c

# How Ray will authenticate with newly launched nodes.
auth:
    ssh_user: ubuntu
# By default Ray creates a new private keypair, but you can also use your own.
# If you do so, make sure to also set "KeyName" in the head and worker node
# configurations below.
#    ssh_private_key: /path/to/your/key.pem

# Provider-specific config for the head node, e.g. instance type. By default
# Ray will auto-configure unspecified fields such as SubnetId and KeyName.
# For more documentation on available fields, see:
# http://boto3.readthedocs.io/en/latest/reference/services/ec2.html#EC2.ServiceResource.create_instances
head_node:
    InstanceType: r5.xlarge  # FIXME: should downgrade this once "light heads" work in Ray
    ImageId: ami-XXX  # Ray+Singularity+ASNets image

    # You can provision additional disk space with a conf as follows
    BlockDeviceMappings:
        - DeviceName: /dev/sda1
          Ebs:
              VolumeSize: 64

    CpuOptions:
        # Takes us down to 8 "real" cores on a 16-thread machine. Not so bad if
        # we only have 128G RAM, since that gives us 16G per proc to play with.
        # Also, going down to 8 cores doesn't save us any money if we want 100G+
        # RAM. Guess AWS is just expensive :P
        CoreCount: 2
        ThreadsPerCore: 1

# Same deal for worker nodes
worker_nodes:
    InstanceType: r5.12xlarge  # 48 threads & 384G memory (about $1/hr)
    ImageId: ami-XXX  # as above

    InstanceMarketOptions:
        MarketType: spot

    CpuOptions:
        # Takes us down to 8 "real" cores on a 16-thread machine. Not so bad if
        # we only have 128G RAM, since that gives us 16G per proc to play with.
        # Also, going down to 8 cores doesn't save us any money if we want 100G+
        # RAM. Guess AWS is just expensive :P
        CoreCount: 24
        ThreadsPerCore: 1

# Files or directories to copy to the head and worker nodes. The format is a
# dictionary from REMOTE_PATH: LOCAL_PATH, e.g.
file_mounts: {
#    "/path1/on/remote/machine": "/path1/on/local/machine",
#    "/path2/on/remote/machine": "/path2/on/local/machine",
}

# List of commands that will be run before `setup_commands`. If docker is
# enabled, these commands will run outside the container and before docker
# is setup.
initialization_commands: []

# List of shell commands to run to set up nodes.
setup_commands:
    # sometimes scripts don't cd properly and do run_asnets in ~/; the following
    # line tries to stop that from happening
    - "echo you should chdir properly into ~/l4p/asnets > ./experiment-results"
    # update repo to latest master
    - cd ~/l4p/ && git checkout master && git pull
    - cd ~/l4p/asnets && pip install -e .
#     # Consider uncommenting these if you also want to run apt-get commands during setup
#     - sudo pkill -9 apt-get || true
#     - sudo pkill -9 dpkg || true
#     - sudo dpkg --configure -a

# Custom commands that will be run on the head node after common setup.
head_setup_commands: []

# Custom commands that will be run on worker nodes after common setup.
worker_setup_commands: []

# Command to start ray on the head node. You don't need to change this.
head_start_ray_commands:
    - ray stop
    # "--num-cpus 0" stops Ray from running stuff on the head node (grrr)
    # FIXME: change back to --num-cpus 0 and use cheaper head node once Ray fixes its shit
    - ulimit -n 65536; ray start --head --num-cpus 1 --redis-port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml

# Command to start ray on worker nodes. You don't need to change this.
worker_start_ray_commands:
    - ray stop
    - ulimit -n 65536; ray start --redis-address=$RAY_HEAD_IP:6379 --object-manager-port=8076
